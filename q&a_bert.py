# -*- coding: utf-8 -*-
"""q&a_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EdpSfzhfFXcmsIajEb4UO-AY2xtKr10o
"""

!pip install torch

!pip install transformers

import pandas as pd
import torch
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline

QA_input = [
  {
  "question":"",
  "context":""
  }
]

model_name = 'deepset/roberta-base-squad2' #hugging face pretrained model

model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

inputs0 = tokenizer(QA_input[0]["question"],QA_input[0]["context"], return_tensors="pt")
output0 = model(**inputs0)

inputs1 = tokenizer(QA_input[1]["question"],QA_input[1]["context"], return_tensors="pt")
output1 = model(**inputs1)

output0

answer_start_idx = torch.argmax(output0.start_logits)
answer_end_idx = torch.argmax(output0.end_logits)

answer_tokens = inputs0.input_ids[0, answer_start_idx: answer_end_idx + 1]
answer = tokenizer.decode(answer_tokens)

print("ques: {}\nanswer: {}".format(QA_input[0]["question"], answer))

answer_start_idx = torch.argmax(output1.start_logits)
answer_end_idx = torch.argmax(output1.end_logits)

answer_tokens = inputs1.input_ids[0, answer_start_idx: answer_end_idx + 1]
answer = tokenizer.decode(answer_tokens)

print("ques: {}\nanswer: {}".format(QA_input[1]["question"], answer))

# method 2

qa = pipeline("question-answering", model=model_name, tokenizer=model_name)

output0 = qa(QA_input[0]["question"], QA_input[0]["context"])
print(output0)

output1 = qa(QA_input[1]["question"], QA_input[1]["context"])
print(output1)
